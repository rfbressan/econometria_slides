---
title: "Econometria I"
subtitle: "Regressão Linear Simples"
author: "Rafael Bressan"
date: "UDESC/Esag </br> `r Sys.Date()`"
output:
  xaringan::moon_reader:
    # chakra: "https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"
    lib_dir: libs
    css: [default, "../css/scpo.css", "../css/scpo-fonts.css"]
    nature:
      beforeInit: ["../js/ru_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      in_header: "../libs/partials/header.html"
---

layout: true

<div class="my-footer"><img src="../img/logo/Esag.jpeg" style="height: 60px;"/></div> 

---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  dev = "svg",
  cache = TRUE,
  fig.align = "center"
  #fig.width = 11,
  #fig.height = 5
)

library("tidyverse")
library("kableExtra")
library("gridExtra")
library("haven")
library("ggpubr")
library("huxtable")
```



# Hoje - Econometria de verdade finalmente `r emo::ji("v")`

* Introdução ao ***Modelo de Regressão Linear Simples*** e ***Mínimos Quadrados Ordinários (MQO)*** *estimativa*.

* Aplicação empírica: *tamanho da turma* e *desempenho do aluno*

---

# Tamanho da turma e desempenho dos alunos

* Quais políticas *conduzem* a um melhor aprendizado dos alunos?

* A redução do tamanho das turmas está no centro dos debates sobre políticas educacionais há *décadas*.

--

* Usaremos dados de um artigo famoso de [Joshua Angrist e Victor Lavy (1999)](https://economics.mit.edu/files/8273).

* Consiste em resultados de testes e características de classe/escola para alunos da quinta série (10-11 anos) em escolas primárias públicas judaicas em Israel em 1991.

* Testes nacionais mediram habilidades de *matemática* e (hebraico) de *leitura*. As pontuações brutas foram dimensionadas de 1 a 100.

---

class:: inverse

# Tarefa 1: Conhecendo os dados

1. Carregue os dados [daqui](https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1) como `grades`. *Dica: Use `read_dta` do pacote `haven` para importar o arquivo, que possui o formato .dta.* (FYI: *.dta* é a extensão de arquivos de dados do [*Stata*](https://www.stata.com/))

1. Descreva o conjunto de dados:

   * Qual é a unidade de observação, ou seja, a que corresponde cada linha?
   * Quantas observações existem?
   * Visualize o conjunto de dados. Que variáveis temos? A que correspondem as variáveis `avgmath` e `avgverb`?
   * Use a função `skim` do pacote `skimr` para obter estatísticas resumidas comuns para as variáveis `classize`, `avgmath` e `avgverb`. (*Dica: use `dplyr` para `selecionar` as variáveis e então simplesmente canalize (pipe `%>%`) `skim()`.*)

1. Calcule a correlação entre o tamanho da turma e as notas de matemática e hebraico. A relação é positiva/negativa, forte/fraca?

```{r, echo=FALSE}
# import data
grades = read_dta(file = "https://www.dropbox.com/s/wwp2cs9f0dubmhr/grade5.dta?dl=1")
```

---

# Tamanho da turma e desempenho: gráfico de dispersão

.pull-left[
```{r, echo=FALSE,fig.height=6}
g_math = ggplot(grades, aes(x = classize, y = avgmath)) + 
    geom_point(size = 2, alpha = 0.5) +
    xlim(0,45) +
    ylim(0, 100) +
    labs(
      x = "Tamanho da turma",
      y = "Nota média",
      title = "Matemática") +
    theme_bw(base_size = 20)
g_math
```
]

.pull-right[
```{r, echo=FALSE,fig.height=6}
g_verb = ggplot(grades, aes(x = classize, y = avgverb)) + 
    geom_point(size = 2, alpha = 0.5) +
    xlim(0,45) +
    ylim(0, 100) +
    labs(x = "Tamanho da turma",
         y = "Nota média",
         title = "Hebraico") +
    theme_bw(base_size = 20)
g_verb
```
]

--

* Associação um tanto positiva conforme sugerido pelas correlações. Vamos calcular a pontuação média por tamanho de turma para ver as coisas com mais clareza!

---

# Tamanho da turma e desempenho: gráfico de dispersão agrupado

.pull-left[
```{r, echo=FALSE,fig.height=6}
grades_avg_cs <- grades %>%
  group_by(classize) %>%
  summarise(avgmath_cs = mean(avgmath),
            avgverb_cs = mean(avgverb))

g_math_cs = ggplot(grades_avg_cs, aes(x = classize, y = avgmath_cs)) + 
    geom_point(size = 2) +
    xlim(0,45) +
    ylim(0, 100) +
    labs(
      x = "Tamanho da turma",
      y = "Nota média",
      title = "Matemática") +
    theme_bw(base_size = 20)
g_math_cs
```
]

.pull-right[
```{r, echo=FALSE,fig.height=6}
g_verb_cs = ggplot(grades_avg_cs, aes(x = classize, y = avgverb_cs)) + 
    geom_point(size = 2) +
    xlim(0,45) +
    ylim(0, 100) +
    labs(x = "Tamanho da turma",
         y = "Nota média",
         title = "Hebraico") +
    theme_bw(base_size = 20)
g_verb_cs
```
]

---

# Tamanho da turma e desempenho: gráfico de dispersão agrupado

* Vamos nos concentrar primeiro nas pontuações de matemática e, para simplificar visualmente, ampliaremos

```{r, echo=FALSE,fig.height=4, fig.width = 8}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14)
```

---

# Tamanho da turma e desempenho: linha de regressão

Como resumir visualmente a relação: **uma linha através do gráfico de dispersão**

--

.left-wide[
```{r,echo=FALSE,fig.align='left',fig.height=4,fig.width=7}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
  geom_hline(yintercept = 65, col = "red")
```
]

--

.right-thin[
<br>

* Uma *linha*! Excelente. Mas **qual** linha? Esta?

* Essa é uma linha *plana*. Mas a pontuação média em matemática *aumenta* com o tamanho da turma `r emo::ji("weary")`

]

---

# Tamanho da turma e desempenho: linha de regressão

Como resumir visualmente a relação: **uma linha através do gráfico de dispersão**

.left-wide[
```{r,echo=FALSE,fig.align='left',fig.height=4,fig.width=7}
g_math_cs +
  ylim(50, 80) +
  theme_bw(base_size = 14) +
  geom_abline(intercept = 55,slope = 0.6, col = "red")
```
]

.right-thin[
<br>

* **Aquela**?

* Um pouco melhor! Tem um **coeficiente angular** e um **intercepto** `r emo::ji("neutral_face")`

* Precisamos de uma regra para decidir! 

]


---

# Regressão Linear Simples

Vamos formalizar um pouco o que estamos fazendo até agora.

* Estamos interessados na relação entre duas variáveis:

--

   * uma __variável de resultado__ (também chamada de __variável dependente__):
   *pontuação média em matemática* $(y)$
  
--
  
   * uma __variável explicativa__ (também chamada de __variável independente__ ou __regressor__):
   *tamanho da turma* $(x)$
  
--

* Para cada classe $i$ observamos o par $\{(x_i, y_i)\}_{i\in\mathcal{I}}$ e, portanto, podemos traçar a *distribuição conjunta* do tamanho da classe e da pontuação média em matemática.

--

* Resumimos essa relação com uma linha (por enquanto). A equação para tal linha com um intercepto $b_0$ e uma inclinação (ou coeficiente angular) $b_1$ é:
     $$
     \widehat{y}_i = b\_0 + b\_1 x\_i
     $$

--

* $\widehat{y}_i$ é nossa *previsão* para $y$ na observação $i$ $(y_i)$ dado nosso modelo (ou seja, a linha).

---

# Regressão Linear Simples: Resíduos

* Se todos os pontos estivessem __sobre__ a linha, então $\widehat{y}_i = y_i$, e não haveria resíduos.

--

```{r, echo = FALSE, fig.height = 4,fig.width=7}
x <- runif(50, min  = 0, max = 1)
y <- 1 * x

data <- data.frame(y = y,
                   x = x)

plot_ex <- data %>% ggplot(aes(x = x, y = y)) +
  geom_point() +
  xlim(0, 1) +
  ylim(0, 1) +
  labs(x = "x",
       y = "y") +
  theme_bw(base_size = 14)
plot_ex
```

---

# Regressão Linear Simples: Resíduos

* Se todos os pontos estivessem __sobre__ a linha, então $\widehat{y}_i = y_i$, e não haveria resíduos.

```{r, echo = FALSE, fig.height = 4,fig.width=7}
plot_ex + geom_line(color = "red")
```

---

# Regressão Linear Simples: Resíduos

* Se todos os pontos estivessem __sobre__ a linha, então $\widehat{y}_i = y_i$, e não haveria resíduos.

* No entanto, como na maioria dos casos a *variável dependente* $(y)$ não é explicada *apenas*  pela *variável independente* escolhida $(x)$, $\widehat{y}_i \neq y_i$, ou seja, exite um __erro__ de previsão.
Este __erro__<sup>1</sup> é chamado de __resíduo__.

.footnote[
[1]: Aqui o termo erro não se refere ao erro populacional, outro conceito.
]

--

* No ponto $(x_i,y_i)$, denotamos esse resíduo por $e_i$ .

--

* Os *dados reais* $(x_i,y_i)$ podem ser escritos como *previsão + resíduo*:
  $$
  y_i = \widehat y_i + e_i \implies y_i = b_0 + b_1 x_i + e_i
  $$


---

# Regressão linear simples: graficamente

```{r, echo = F, fig.width = 10, fig.height = 5}
plot_1 <- g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14)
plot_1
```

---

# Regressão linear simples: graficamente

```{r, echo = F, fig.width = 10, fig.height = 5}
plot_2 <- plot_1 +
  stat_smooth(data = grades_avg_cs, method = "lm", se = FALSE, 
              colour = "red") +
  annotate("text", x = 45, y = 70, label = "hat(y)", parse = TRUE, 
           colour = "red", size = 6)
plot_2
```

---

# Regressão linear simples: graficamente

```{r, echo = F, fig.width = 10, fig.height = 5}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
  stat_smooth(method = "lm", se = FALSE, colour = "red") +
  annotate("text", x = 45, y = 70, label = "hat(y)", parse = TRUE, 
           colour = "red", size = 6) +
  geom_point(data = grades_avg_cs %>% filter(classize == 17), 
             aes(x = classize, y = avgmath_cs), color = "red", size = 4) +
  annotate("text", x = 17, y = 69, label = "y[x = 17]", parse = TRUE, 
           colour = "red", size = 6)
```

---

# Regressão linear simples: graficamente

```{r, echo = F, fig.width = 10, fig.height = 5}
math_class_reg <- lm(avgmath_cs ~ classize, data = grades_avg_cs)
math_class_reg <- broom::augment(math_class_reg)

g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
  stat_smooth(method = "lm", se = FALSE, colour = "red") +
  annotate("text", x = 45, y = 70, label = "hat(y)", parse = TRUE, colour = "red", size = 6) +
  geom_point(data = grades_avg_cs %>% filter(classize == 17), aes(x = classize, y = avgmath_cs), color = "red", size = 4) +
  annotate("text", x = 17, y = 69, label = "y[x = 17]", parse = TRUE, colour = "red", size = 6) +
  geom_segment(data = math_class_reg %>% filter(classize == 17),
               aes(xend = classize, yend = .fitted), color = "red", size = 1) +
  annotate("text", x = 18, y = 65.55, label = "e[x = 17]", parse = TRUE, colour = "red", size = 6)
```

---

# Regressão linear simples: graficamente

```{r, echo = F, fig.width = 10, fig.height = 5}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
  stat_smooth(method = "lm", se = FALSE, colour = "red") +
  annotate("text", x = 45, y = 70, label = "hat(y)", parse = TRUE, colour = "red", size = 6) +
  geom_segment(data = math_class_reg,
               aes(xend = classize, yend = .fitted), color = "red", size = 0.5)
```

---

# Regressão linear simples: graficamente

.left-wide[
```{r, echo = F, out.width = "100%", fig.height = 4.5}
g_math_cs +
    ylim(50, 80) +
    theme_bw(base_size = 14) +
  stat_smooth(method = "lm", se = FALSE, colour = "red") +
  annotate("text", x = 45, y = 70, label = "hat(y)", parse = TRUE, 
           colour = "red", size = 6) +
  geom_segment(data = math_class_reg,
               aes(xend = classize, yend = .fitted), color = "red", size = 0.5)
```
]

.right-thin[
<br>
<br>
.udescgreen.big.center[**Qual critério de ajuste utilizar?**]
]
---

# **M**ínimos **Q**uadrados **O**rdinários (MQO)

--

* Erros de sinal diferente $(+/-)$ se cancelam, então consideramos **resíduos ao quadrado**
$$\forall i \in \mathcal{I} = [1, \ldots, N], \quad e_i^2 = (y_i - \widehat y_i)^2 = (y_i - b_0 - b_1 x_i)^2$$

--
.pull-left[
<br>
* Escolha $(b_0,b_1)$ tal que: 

$$(b_0,b_1) = \underset{b_0, b_1}{\arg\min}\sum_{i = 1}^N e_1^2 + \dots + e_N^2$$
]

--

.pull-right[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=7,fig.height = 3.5}
g_math_cs +
    ylim(50, 80) +
    xlim(0, 50) +
    theme_bw(base_size = 14) +
    stat_smooth(method = "lm", se = FALSE, colour = "darkgreen") +
  coord_fixed(ratio = 0.65)
```
]

---

# **M**ínimos **Q**uadrados **O**rdinários (MQO)

* Escolha $(b_0,b_1)$ tal que $\sum_{i = 1}^N e_1^2 + \dots + e_N^2$ seja **o mínimo possível**.

```{r, echo=FALSE, message=FALSE, warning=FALSE,fig.width=7,fig.height = 3.5}
g_math_cs +
    ylim(50, 80) +
    xlim(0, 50) +
    theme_bw(base_size = 14) +
    stat_smooth(method = "lm", se = FALSE, colour = "darkgreen") +
    geom_rect(data = math_class_reg,
              aes(
              xmin = classize,
              xmax = classize + abs(.resid)*0.65,
              ymin = avgmath_cs,
              ymax = avgmath_cs - .resid),
              fill = "darkgreen",
              alpha = 0.3) +
  coord_fixed(ratio = 0.65)
```

---

# **M**ínimos **Q**uadrados **O**rdinários (MQO)

```{r, echo = F, out.extra = 'style="border: none;"'}
knitr::include_url("https://gustavek.shinyapps.io/reg_simple/")
```

---

# **M**ínimos **Q**uadrados **O**rdinários (MQO)

```{r, echo = F, out.extra = 'style="border: none;"'}
knitr::include_url("https://gustavek.shinyapps.io/SSR_cone/")
```

---

# **M**ínimos **Q**uadrados **O**rdinários (MQO): Coeficientes

* **MQO**: método de *estimativa* que consiste em minimizar a soma dos quadrados dos resíduos.

* Produz soluções __únicas__ para este problema de minização. .udescgreen[_Como?_]

* Então, quais são as fórmulas para $b_0$ (intercepto) e $b_1$ (inclinação)?

--

* Em nosso caso de variável independente única:

> ### __Inclinação: $b_1^{OLS} = \frac{cov(x,y)}{var(x)}$ $\hspace{2cm}$ Intercepto: $b_0^{OLS} = \bar{y } - b_1\bar{x}$__

--

* Essas fórmulas não surgem como mágica. Elas podem ser encontradas resolvendo a minimização de erros quadráticos. A matemática pode ser encontrada [aqui](https://www.youtube.com/watch?v=Hi5EJnBHFB4) para quem estiver interessado.

---

# **M**ínimos **Q**uadrados **O**rdinários (MQO): Interpretação

Por enquanto, suponha que tanto a variável dependente $(y)$ quanto a variável independente $(x)$ sejam numéricas.

--

> Intercepto $(b_0)$: **O valor previsto de $y$ quando $x = 0$.**

--

> Inclinação $(b_1)$: **A mudança prevista, em média, no valor de $y$ *associada* a um aumento de uma unidade em $x$.**

--

* `r emo::ji("warning")` Observe que usamos o termo *associada*, **claramente evitando interpretar $b_1$ como o impacto causal de $x$ em $y$**. Para fazer tal afirmação, precisamos que algumas condições específicas sejam atendidas. Este é o ramo da econometria que trata de ***inferência causal***.

--
 
* Observe também que a unidade de $x$ é importante para a interpretação (e magnitude!) de $b_1$.

--

* **Você precisa ser explícito sobre qual é a unidade de $x$!**

---

# MQO com o `R`

* Em `R`, as regressões OLS são estimadas usando a função `lm`.

* É assim que funciona:

  ```{r, echo = TRUE, eval = FALSE}
  lm(formula = dep_var ~  indep_var, data = df)
  ```
--

## Tamanho da turma e desempenho dos alunos

Vamos estimar o seguinte modelo por MQO: $\textrm{mtm_media}_i = b_0 + b_1 \textrm{tamanho}_i + e_i$

.pull-left[
```{r echo=T, eval = FALSE}
# Regressão MQO da pontuação média de matemática 
# no tamanho da turma 
lm(avgmath_cs ~ classize, grades_avg_cs) 
```
]

.pull-right[
```{r echo=F, eval = TRUE}
# Regressão MQO da pontuação média de matemática 
# no tamanho da turma
lm(avgmath_cs ~ classize, grades_avg_cs) 
```
]

---

# **M**ínimos **Q**uadrados **O**rdinários (MQO): Previsão

```{r echo=F, eval = TRUE}
# Regressão MQO da pontuação média de matemática 
# no tamanho da turma
lm(formula = avgmath_cs~classize, data = grades_avg_cs)
```

--

Isso implica (abstraindo o subscrito $_i$ para simplificar):


$$
\begin{align}
\widehat y &= b_0 + b_1 x \\
\widehat {\text{média de matemática}} &= b_0 + b_1 \cdot \text{tamanho da turma} \\
\widehat {\text{média de matemática}} &= 61,11 + 0,19 \cdot \text{tamanho da turma}
\end{align}
$$

--

Qual é a pontuação média prevista para uma turma de 26 alunos? (Usando os coeficientes *econtrados*.)

$$
\begin{aligned}
\widehat {\text{média de matemática}} &= 61.11 + 0.19 \cdot 26 \\
\widehat {\text{média de matemática}} &= 66.08
\end{aligned}
$$

---

class: inverse

# Tarefa 2: Regressão MQO


Execute o seguinte código para agregar os dados no nível de tamanho da classe:

```{r, eval = F}
grades_avg_cs <- grades %>%
  group_by(classize) %>%
  summarise(avgmath_cs = mean(avgmath),
            avgverb_cs = mean(avgverb))
```

1. Regredir a pontuação verbal média (variável dependente) no tamanho da turma (variável independente). Interprete os coeficientes.

1. Calcule os coeficientes MQO $b_0$ e $b_1$ da regressão anterior usando as fórmulas do slide 25. (*Dica:* você precisa usar as funções `cov`, `var` e `mean`.)

1. Rode a regressão com a função `lm` e compare os coeficientes obtidos.

1. Qual é a pontuação verbal média prevista quando o tamanho da turma é igual a 0? (Isso faz sentido?!)

1. Qual é a pontuação verbal média prevista quando o tamanho da turma é igual a 30 alunos?
---

# Previsões e Resíduos: Propriedades

.pull-left[
* __A média (ou soma) dos resíduos é 0.__
   $$\begin{align} \frac{1}{N} \sum_{i=1}^N e_i &= \frac{1}{N} \sum_{i=1}^N (y_i - \widehat y_i ) \\ &= \bar{y} - \frac{1}{N} \sum_{i=1}^N \widehat{y}_i \\\ &= 0 \end{align}$$

* __A média de $\widehat{y}_i$ é igual a $\bar{y}$.__
     $$\begin{align} \frac{1}{N} \sum_{i=1}^N \widehat{y}_i &= \frac{1}{N} \sum_{i=1}^N b_0 + b_1 x_i \\ &= b_0 + b_1 \bar{x} = \bar{y} \end{align}$$

]
--

.pull-right[
* __ Regressor e resíduos não são correlacionados (por imposição).__


   $$Cov(x_i, e_i) = 0$$


* __Previsão e resíduos não são correlacionados.__

   $$\begin{align} Cov(\widehat y_i, e_i) &= Cov(b_0 + b_1x_i, e_i) \\ &= b_1Cov(x_i,e_i) \\ &= 0 \end{align}$$
  
   Já que $Cov(a + bx, y) = bCov(x,y)$.

]

---

# Suposição de Linearidade: Visualize seus Dados!

* É importante ter em mente que covariância, correlação e regressão simples MQO medem apenas **relações lineares** entre duas variáveis.

* Dois conjuntos de dados com correlações *idênticas* e linhas de regressão podem parecer *muito* diferentes.

---

# Suposição de linearidade: Anscombe

* Francis Anscombe (1973) apresentou 4 conjuntos de dados com estatísticas idênticas. Mas olhe!

.left-wide[

```{r,echo=FALSE,fig.height = 4}
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
covs = data.frame(dataset = 1:4, cov = 0.0)
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  covs[i,"cov"] = eval(parse(text = paste0("cov(anscombe$x",i,",anscombe$y",i,")")))
  covs[i,"var(y)"] = eval(parse(text = paste0("var(anscombe$y",i,")")))
  covs[i,"var(x)"] = eval(parse(text = paste0("var(anscombe$x",i,")")))
}

op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 0, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "black", pch = 21, bg = "red", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13),main=paste("dataset",i))
  abline(mods[[i]], col = "#DE9854")
}
par(op)
```
]

--

.right-thin[
</br>
</br>

```{r,echo = FALSE}
ch = kable(round(covs,3))
ch %>%
   kable_styling(bootstrap_options = "striped", font_size = 20)
```

]
---

# Relações não-lineares nos dados?

.pull-left[
* Podemos acomodar relações não lineares em regressões.

* Basta adicionar um termo de *ordem superior* como este:
     $$
     y_i = b_0 + b_1 x_i + b_2 x_i^2 + e_i
     $$
    
* Esta é uma __regressão múltipla__ (em 2 semanas!)
]
--

.pull-right[
* Por exemplo, suponha que tenhamos esses dados e ajustemos o modelo de regressão anterior:
    ```{r non-line-cars-ols2,echo=FALSE,echo=FALSE,fig.height = 6}
data(mtcars)
mtcars %>% ggplot(aes(x = hp, y = mpg)) +
    geom_point() +
    stat_smooth(method='lm', formula = y~poly(x,2), se = FALSE, aes(colour="Não-linear")) +
    stat_smooth(method='lm', se = FALSE, aes(colour="Linear")) +
    scale_colour_manual(name="legend", values=c("darkgreen", "red")) +
    labs(x = "x",
         y = "y",
         title = "Relação não-linear entre x e y") +
    theme_bw(base_size = 20) +
    theme(legend.position="top") +
    theme(legend.title = element_blank())

    ```
]

---

# Análise de variância - ANOVA

* Lembre-se que $y_i = \widehat{y}_i + e_i$.

* Temos a seguinte decomposição:
    $$\begin{align} Var(y) &= Var(\widehat{y} + e)\\&= Var(\widehat{y}) + Var(e) + 2 Cov(\widehat{y}, e)\\&= Var(\widehat{y}) + Var(e)\end{align}$$
    

* __Variação total (SST) = explicado pelo modelo(SSE) + Inexplicável (SSR)__

---

# Qualidade de ajuste

* O __ $R^2$ __ mede quão bem o __model se ajusta aos dados__.

--

$$
R^2 = \frac{\text{variância explicada}}{\text{variância total}} = \frac{SSE}{SST} = 1 - \frac{SSR}{SST}\in[0,1]
$$

--
    
* $R^2$ próximo de $1$ indica um poder explicativo __muito ***alto***__ do modelo.

* $R^2$ próximo a $0$ indica um poder explicativo __muito ***baixo***__ do modelo.

--

* *Interpretação:* um $R^2$ de 0,5, por exemplo, significa que a variação em $x$ "explica" 50% da variação em $y$.

--
    
* `r emo::ji("warning")` Baixo $R^2$ __NÃO__ significa que um modelo é inútil! Lembre-se de que a econometria está interessada muitas vezes em ***mecanismos causais***, não necessariamente em previsão!

--
    
* `r emo::ji("warning")` O $R^2$ __NÃO__ é um indicador de um relacionamento causal!

---

class: inverse

# Tarefa 3: $R^2$ e qualidade de ajuste


1. Regresse `avgmath_cs` em `classize`. Atribua a um objeto `math_reg`.

1. Passe `math_reg` na função `summary()`. Qual é o (múltiplo) $R^2$ para esta regressão? Como você pode interpretá-lo?

1. Calcule a correlação ao quadrado entre `classize` e `avgmath_cs`. O que isso lhe diz sobre a relação entre $R^2$ e a correlação em uma regressão com apenas um regressor?

1. Repita os passos 1 e 2 para `avgverb_cs`. Para qual exame a variação no tamanho da turma explica mais a variação nas notas dos alunos?

1. (Opcional) Instale e carregue o pacote `broom`. Passe `math_reg` na função `augment()` e atribua-o a um novo objeto. Use a variação em `avgmath_cs` (SST) e a variação em `.fitted` (valores previstos; SSE) para encontrar o $R^2$ usando a fórmula do slide anterior.

---


layout: false

class: title-slide-final, middle
background-image: url(../img/logo/UdescEsag.jpeg)
background-size: 350px
background-position: 9% 9%

# ATÉ A PRÓXIMA AULA!

.footnote[
[1]: Este slides foram baseados nas aulas de econometria da [SciencesPo Department of Economics](https://github.com/ScPoEcon/ScPoEconometrics-Slides)
]


|                                                                                                            |                                   |
| :--------------------------------------------------------------------------------------------------------- | :-------------------------------- |
| <a href="https://github.com/rfbressan/econometria_slides">.ScPored[<i class="fa fa-link fa-fw"></i>] | Slides |
| <a href="http://github.com/rfbressan">.ScPored[<i class="fa fa-github fa-fw"></i>] | @rfbressan |
